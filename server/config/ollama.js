import dotenv from 'dotenv';

// Ensure environment variables are loaded
dotenv.config();

// Ollama configuration
const OLLAMA_BASE_URL = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
const OLLAMA_NGROK_URL = process.env.OLLAMA_NGROK_URL || null;
const OLLAMA_MODEL = 'llama3.1';

// Determine which URL to use (prefer ngrok if available)
const ACTIVE_OLLAMA_URL = OLLAMA_NGROK_URL || OLLAMA_BASE_URL;

console.log('🦙 Ollama Configuration:');
console.log(`   Local URL: ${OLLAMA_BASE_URL}`);
console.log(`   Ngrok URL: ${OLLAMA_NGROK_URL || 'Not configured'}`);
console.log(`   Active URL: ${ACTIVE_OLLAMA_URL}`);
console.log(`   Model: ${OLLAMA_MODEL}`);

// Test Ollama connection
let ollamaAvailable = false;

const testOllamaConnection = async () => {
  // Test ngrok URL first if available, then fallback to local
  const urlsToTest = OLLAMA_NGROK_URL ? [OLLAMA_NGROK_URL, OLLAMA_BASE_URL] : [OLLAMA_BASE_URL];

  for (const url of urlsToTest) {
    try {
      console.log(`🔍 Testing Ollama connection at: ${url}`);

      // For ngrok URL, use the /api/generate endpoint
      const testEndpoint = url.includes('ngrok') ? `${url}` : `${url}/api/tags`;

      if (url.includes('ngrok')) {
        // Test ngrok endpoint with a simple generation request
        const testResponse = await fetch(testEndpoint, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            model: OLLAMA_MODEL,
            prompt: 'Hello',
            stream: false
          })
        });

        if (testResponse.ok) {
          ollamaAvailable = true;
          console.log(`✅ Ollama ngrok connection successful at ${url}`);
          return;
        }
      } else {
        // Test local endpoint with tags
        const response = await fetch(testEndpoint);
        if (response.ok) {
          const data = await response.json();
          const models = data.models || [];
          const hasLlama31 = models.some(model => model.name.includes('llama3.1'));

          if (hasLlama31) {
            ollamaAvailable = true;
            console.log(`✅ Ollama local connection successful - llama3.1 model available at ${url}`);
            return;
          } else {
            console.warn(`⚠️  Ollama connected at ${url} but llama3.1 model not found. Available models:`, models.map(m => m.name));
          }
        }
      }
    } catch (error) {
      console.warn(`⚠️  Could not connect to Ollama at ${url} - Error:`, error.message);
    }
  }

  console.warn('⚠️  Could not connect to any Ollama endpoint. Make sure Ollama is running and accessible.');
};

// Test connection on startup
testOllamaConnection();

/**
 * Generate AI response using Ollama
 */
export const generateOllamaResponse = async (messages, guru, options = {}) => {
  try {
    // Check if Ollama is available
    if (!ollamaAvailable) {
      return {
        content: 'AI features are currently unavailable. Please ensure Ollama is running and llama3.1 model is installed.',
        metadata: {
          model: 'unavailable',
          tokens: 0,
          processingTime: 0,
          error: 'Ollama not available or llama3.1 model not found'
        }
      };
    }

    const {
      temperature = guru.settings?.temperature || 0.7,
      maxTokens = guru.settings?.maxTokens || 1024,
    } = options;

    // Prepare messages for Ollama API
    const ollamaMessages = [
      {
        role: 'system',
        content: guru.systemPrompt
      },
      ...messages.map(msg => ({
        role: msg.sender === 'user' ? 'user' : 'assistant',
        content: msg.content
      }))
    ];

    const startTime = Date.now();

    // Make request to Ollama API (use ngrok URL if available, otherwise local)
    let response;

    if (OLLAMA_NGROK_URL) {
      // Use ngrok endpoint with /api/generate format
      const prompt = ollamaMessages.map(msg => {
        if (msg.role === 'system') return `System: ${msg.content}`;
        if (msg.role === 'user') return `User: ${msg.content}`;
        return `Assistant: ${msg.content}`;
      }).join('\n\n');

      response = await fetch(OLLAMA_NGROK_URL, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: OLLAMA_MODEL,
          prompt: prompt,
          stream: false,
          options: {
            temperature: temperature,
            num_predict: maxTokens,
          }
        })
      });
    } else {
      // Use local endpoint with /api/chat format
      response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: OLLAMA_MODEL,
          messages: ollamaMessages,
          stream: false,
          options: {
            temperature: temperature,
            num_predict: maxTokens,
          }
        })
      });
    }

    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.status} ${response.statusText}`);
    }

    const completion = await response.json();
    const processingTime = Date.now() - startTime;

    // Handle different response formats (ngrok vs local)
    let content, tokens, promptTokens, completionTokens;

    if (OLLAMA_NGROK_URL) {
      // Ngrok endpoint returns response in 'response' field
      content = completion.response || 'I apologize, but I could not generate a response.';
      tokens = completion.eval_count || 0;
      promptTokens = completion.prompt_eval_count || 0;
      completionTokens = completion.eval_count || 0;
    } else {
      // Local endpoint returns response in 'message.content' field
      content = completion.message?.content || 'I apologize, but I could not generate a response.';
      tokens = completion.eval_count || 0;
      promptTokens = completion.prompt_eval_count || 0;
      completionTokens = completion.eval_count || 0;
    }

    return {
      content: content,
      metadata: {
        model: OLLAMA_MODEL,
        tokens: tokens,
        processingTime: processingTime,
        promptTokens: promptTokens,
        completionTokens: completionTokens,
        endpoint: OLLAMA_NGROK_URL ? 'ngrok' : 'local'
      }
    };
  } catch (error) {
    console.error('Ollama API error:', error);

    // Return a fallback response
    return {
      content: 'I apologize, but I\'m experiencing technical difficulties. Please try again in a moment.',
      metadata: {
        model: 'fallback',
        tokens: 0,
        processingTime: 0,
        error: error.message
      }
    };
  }
};

/**
 * Get available Ollama models
 */
export const getAvailableModels = () => {
  return [
    {
      id: 'llama3.1',
      name: 'Llama 3.1 (Local)',
      description: 'Local Llama 3.1 model running on Ollama',
      maxTokens: 4096,
      recommended: true,
      local: true
    }
  ];
};

/**
 * Validate model settings
 */
export const validateModelSettings = (settings) => {
  const availableModels = getAvailableModels().map(m => m.id);
  
  return {
    model: availableModels.includes(settings.model) ? settings.model : 'llama3.1',
    temperature: Math.max(0, Math.min(2, settings.temperature || 0.7)),
    maxTokens: Math.max(1, Math.min(4096, settings.maxTokens || 1024)),
    topP: Math.max(0, Math.min(1, settings.topP || 1))
  };
};

/**
 * Test Ollama connection and model availability
 */
export const testOllamaConnectionStatus = async () => {
  try {
    if (OLLAMA_NGROK_URL) {
      // Test ngrok endpoint with a simple generation
      const response = await fetch(OLLAMA_NGROK_URL, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: OLLAMA_MODEL,
          prompt: 'Test',
          stream: false
        })
      });

      if (response.ok) {
        return {
          connected: true,
          endpoint: 'ngrok',
          url: OLLAMA_NGROK_URL,
          model: OLLAMA_MODEL
        };
      }
      return { connected: false, error: `HTTP ${response.status}`, endpoint: 'ngrok' };
    } else {
      // Test local endpoint
      const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`);
      if (response.ok) {
        const data = await response.json();
        const models = data.models || [];
        return {
          connected: true,
          endpoint: 'local',
          url: OLLAMA_BASE_URL,
          models: models.map(m => m.name),
          hasLlama31: models.some(model => model.name.includes('llama3.1'))
        };
      }
      return { connected: false, error: `HTTP ${response.status}`, endpoint: 'local' };
    }
  } catch (error) {
    return { connected: false, error: error.message, endpoint: OLLAMA_NGROK_URL ? 'ngrok' : 'local' };
  }
};

/**
 * Simple test function to verify Ollama integration
 */
export const testOllamaIntegration = async () => {
  try {
    const testGuru = {
      systemPrompt: "You are a helpful AI assistant. Respond briefly and clearly.",
      settings: {
        temperature: 0.7,
        maxTokens: 100
      }
    };

    const testMessages = [{
      sender: 'user',
      content: 'Hello! Please respond with "Ollama integration working!" to confirm the connection.'
    }];

    const response = await generateOllamaResponse(testMessages, testGuru);
    return {
      success: true,
      response: response.content,
      metadata: response.metadata
    };
  } catch (error) {
    return {
      success: false,
      error: error.message
    };
  }
};

// For backward compatibility, export generateOllamaResponse as generateGroqResponse
export const generateGroqResponse = generateOllamaResponse;

export default {
  generateOllamaResponse,
  generateGroqResponse,
  getAvailableModels,
  validateModelSettings,
  testOllamaConnectionStatus,
  testOllamaIntegration
};
